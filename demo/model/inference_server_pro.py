# inference_server_pro.py
# è¿™æ˜¯ä¸€ä¸ªå¢å¼ºç‰ˆæœåŠ¡ï¼šæ”¯æŒå•å¼ é¢„æµ‹ + æ‰¹é‡å¼‚æ­¥é¢„æµ‹ + GPUç›‘æ§ + è¿›åº¦æ¡
import os
import io
import time
import uuid
import threading
import torch
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet50
from flask import Flask, request, jsonify
from PIL import Image

# å°è¯•å¯¼å…¥ GPU ç›‘æ§åº“ (å¦‚æœå®‰è£…å¤±è´¥ä¹Ÿä¸å½±å“ä¸»ç¨‹åºè¿è¡Œ)
try:
    import pynvml
    pynvml.nvmlInit()
    HAS_GPU_MONITOR = True
except Exception as e:
    HAS_GPU_MONITOR = False
    print(f"âš ï¸ GPU Monitor disabled (nvidia-ml-py3 not found): {e}")

# ================= é…ç½®åŒºåŸŸ =================
MODEL_PATH = '/home/hzcu/repo/modelStaff/ResNet50_v1.pth' 
FEEDBACK_FOLDER = '/home/hzcu/repo/modelStaff/feedback_data'
PORT = 5002  # ä½¿ç”¨ 5002 ç«¯å£ï¼Œé¿å…å†²çª

# ================= æ ¸å¿ƒé€»è¾‘ =================
app = Flask(__name__)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TASKS_DB = {} # å…¨å±€å­—å…¸ï¼Œç”¨äºå­˜å‚¨æ‰¹é‡ä»»åŠ¡çš„çŠ¶æ€

# --- 1. ç±»åˆ«å®šä¹‰ (ä¿æŒä¸åŸç‰ˆç»å¯¹ä¸€è‡´) ---
raw_classes = [
    'Apple_Black_Rot', 'Apple_Cedar_Apple_Rust', 'Apple_healthy', 'Apple_Scab', 
    'Blueberry_healthy', 'Cherry_healthy', 'Cherry_Powdery_Mildew', 
    'Corn_Common_Rust', 'Corn_Gray_Leaf_Spot', 'Corn_healthy', 
    'Corn_Northern_Leaf_Blight', 'Grape_Black_Rot', 'Grape_Esca_Black_Measles', 
    'Grape_healthy', 'Grape_Leaf_Blight_Isariopsis', 'Orange_Haunglongbing_Citrus_Greening', 
    'Peach_Bacterial_Spot', 'Peach_healthy', 'Pepper_Bell_Bacterial_Spot', 
    'Pepper_Bell_healthy', 'Potato_Early_Blight', 'Potato_healthy', 
    'Potato_Late_Blight', 'Raspberry_healthy', 'Soybean_healthy', 
    'Squash_Powdery_Mildew', 'Strawberry_healthy', 'Strawberry_Leaf_Scorch', 
    'Tomato_Bacterial_Spot', 'Tomato_Early_Blight', 'Tomato_healthy', 
    'Tomato_Late_Blight', 'Tomato_Leaf_Mold', 'Tomato_Mosaic_Virus', 
    'Tomato_Septoria_Leaf_Spot', 'Tomato_Target_Spot', 'Tomato_Two_Spotted_Spider_Mite', 
    'Tomato_Yellow_Leaf_Curl_Virus', 'Wheat_Crown_and_Root_Rot', 'Wheat_healthy', 
    'Wheat_Leaf_Rust', 'Wheat_Loose_Smut'
]
CLASS_NAMES = sorted(raw_classes)
NUM_CLASSES = len(CLASS_NAMES)

# --- 2. åŠ è½½æ¨¡å‹ ---
print(f"ğŸ”„ Loading model from {MODEL_PATH} on {DEVICE}...")
try:
    model = resnet50(weights=None) 
    num_ftrs = model.fc.in_features
    model.fc = torch.nn.Linear(num_ftrs, NUM_CLASSES)
    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
    model.load_state_dict(state_dict)
    model.to(DEVICE)
    model.eval()
    print("âœ… Model loaded successfully!")
except Exception as e:
    print(f"âŒ Error loading model: {e}")

# --- 3. é¢„å¤„ç†å®šä¹‰ ---
transform = T.Compose([
    T.Resize(256),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ================= è¾…åŠ©å·¥å…·ï¼šGPU ç›‘æ§ =================
def get_gpu_usage():
    """ç®¡ç†å‘˜ä¸“ç”¨ï¼šè·å–GPUæ˜¾å­˜å’Œè´Ÿè½½"""
    if not HAS_GPU_MONITOR or DEVICE.type != 'cuda':
        return "GPU: N/A"
    try:
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        used_mb = mem_info.used // 1024**2
        total_mb = mem_info.total // 1024**2
        return f"GPU: {util.gpu}% | Mem: {used_mb}/{total_mb} MB"
    except:
        return "GPU: Err"

# ================= æ‰¹é‡å¤„ç†å·¥å…·ç±» =================
class ImageFolderDataset(Dataset):
    """è‡ªå®šä¹‰ Datasetï¼Œç”¨äºè¯»å–æ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡"""
    def __init__(self, file_paths, transform):
        self.file_paths = file_paths
        self.transform = transform

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        path = self.file_paths[idx]
        try:
            image = Image.open(path).convert('RGB')
            return self.transform(image), path
        except Exception as e:
            print(f"âš ï¸ Reads error {path}: {e}")
            # è¿”å›ä¸€ä¸ªå…¨0çš„tensoré˜²æ­¢æŠ¥é”™ä¸­æ–­ï¼Œåç»­å¯ä»¥æ ¹æ®pathè¿‡æ»¤
            return torch.zeros((3, 224, 224)), "ERROR_FILE"

def run_batch_inference(task_id, folder_path):
    """åå°è¿è¡Œçš„æ‰¹é‡é¢„æµ‹é€»è¾‘"""
    print(f"[{task_id}] Thread started for: {folder_path}")
    
    # 1. æ‰«ææ–‡ä»¶
    valid_exts = ('.jpg', '.jpeg', '.png', '.bmp')
    all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(valid_exts)]
    total = len(all_files)
    
    if total == 0:
        TASKS_DB[task_id]['status'] = 'failed'
        TASKS_DB[task_id]['error'] = 'No images found in folder'
        return

    # 2. ç­–ç•¥é€‰æ‹© (Q3éœ€æ±‚)
    batch_size = 32 if total >= 50 else 1
    num_workers = 4 if os.cpu_count() > 4 else 0
    
    print(f"[{task_id}] Strategy: Total={total}, BatchSize={batch_size}")

    dataset = ImageFolderDataset(all_files, transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    TASKS_DB[task_id].update({'total': total, 'processed': 0, 'status': 'processing'})
    
    results = []
    start_time = time.time()

    # 3. å¼€å§‹é¢„æµ‹
    with torch.no_grad():
        for batch_imgs, batch_paths in dataloader:
            batch_imgs = batch_imgs.to(DEVICE)
            
            outputs = model(batch_imgs)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            confidences, preds = torch.max(probs, 1)
            
            # å¤„ç†è¿™ä¸€ä¸ª Batch çš„ç»“æœ
            current_batch_results = []
            for i in range(len(batch_paths)):
                path = batch_paths[i]
                if path == "ERROR_FILE": continue # è·³è¿‡æŸåå›¾ç‰‡

                idx = preds[i].item()
                conf = confidences[i].item()
                
                res = {
                    "filename": os.path.basename(path),
                    "class_name": CLASS_NAMES[idx],
                    "confidence": float(f"{conf:.4f}")
                }
                current_batch_results.append(res)
            
            results.extend(current_batch_results)
            
            # æ›´æ–°å…¨å±€è¿›åº¦
            processed = len(results)
            elapsed = time.time() - start_time
            avg_per_img = elapsed / processed if processed > 0 else 0
            eta = (total - processed) * avg_per_img
            
            TASKS_DB[task_id].update({
                'processed': processed,
                'progress_percent': int((processed / total) * 100),
                'eta_seconds': int(eta),
                'avg_latency_ms': int(avg_per_img * 1000)
            })
            
            # ç®¡ç†å‘˜æ—¥å¿— (Q1 & Q4)
            gpu_log = get_gpu_usage()
            print(f"[{task_id}] {processed}/{total} ({int(processed/total*100)}%) | ETA: {int(eta)}s | {gpu_log}")

    # 4. å®Œæˆ
    TASKS_DB[task_id]['status'] = 'completed'
    TASKS_DB[task_id]['results'] = results # è¿™é‡Œå­˜äº†æ‰€æœ‰ç»“æœ
    # æ³¨æ„ï¼šå¦‚æœ results æå¤§(å‡ åä¸‡æ¡)ï¼Œå»ºè®®ç›´æ¥å†™å…¥ json æ–‡ä»¶åˆ°ç¡¬ç›˜ï¼Œè€Œä¸æ˜¯å­˜åœ¨å†…å­˜é‡Œ
    print(f"[{task_id}] Finished in {int(time.time() - start_time)}s")


# ================= æ¥å£å®šä¹‰ =================

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "up", "device": str(DEVICE), "mode": "Pro"})

# --- ä¿æŒåŸæœ‰çš„å•å¼ é¢„æµ‹æ¥å£ (å…¼å®¹æ€§) ---
@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files: return jsonify({'error': 'No file'}), 400
    file = request.files['file']
    try:
        img_bytes = file.read()
        image = Image.open(io.BytesIO(img_bytes)).convert('RGB')
        img_tensor = transform(image).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            outputs = model(img_tensor)
            conf, pred = torch.max(torch.nn.functional.softmax(outputs, dim=1), 1)
        return jsonify({
            'prediction': {'class_name': CLASS_NAMES[pred.item()], 'confidence': float(conf.item())},
            'status': 'success'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# --- ä¿æŒåŸæœ‰çš„åé¦ˆæ¥å£ ---
@app.route('/feedback', methods=['POST'])
def save_feedback():
    # ... (åŸæœ‰ä»£ç é€»è¾‘ä¿æŒä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…è¿™é‡Œçœç•¥ï¼Œä½ è‡ªå·±è´´è¿‡æ¥æˆ–è€…è¿™éƒ¨åˆ†ä¸éœ€è¦æ”¹åŠ¨) ...
    # ä¸ºäº†å®Œæ•´è¿è¡Œï¼Œå»ºè®®è¿™é‡Œç›´æ¥å¤åˆ¶åŸæ¥ save_feedback çš„å†…å®¹
    pass 

# +++++++++++++++++ NEW: æ‰¹é‡ä»»åŠ¡æ¥å£ +++++++++++++++++

@app.route('/batch/start', methods=['POST'])
def start_batch_task():
    """Java ä¸Šä¼  Zip è§£å‹åï¼Œè°ƒç”¨æ­¤æ¥å£å¼€å§‹é¢„æµ‹"""
    data = request.json
    folder_path = data.get('folder_path')
    task_id = data.get('task_id')
    
    if not folder_path or not task_id:
        return jsonify({"error": "Missing folder_path or task_id"}), 400
    
    if not os.path.exists(folder_path):
        return jsonify({"error": "Folder does not exist"}), 404

    # åˆå§‹åŒ–çŠ¶æ€
    TASKS_DB[task_id] = {'status': 'pending', 'processed': 0, 'total': 0}
    
    # å¯åŠ¨åå°çº¿ç¨‹
    t = threading.Thread(target=run_batch_inference, args=(task_id, folder_path))
    t.start()
    
    return jsonify({"status": "started", "task_id": task_id})

@app.route('/batch/status/<task_id>', methods=['GET'])
def get_batch_status(task_id):
    """å‰ç«¯è½®è¯¢æ­¤æ¥å£è·å–è¿›åº¦æ¡"""
    task = TASKS_DB.get(task_id)
    if not task:
        return jsonify({"error": "Task not found"}), 404
        
    response = {
        "status": task['status'],
        "progress": task.get('progress_percent', 0),
        "metrics": {
            "processed": task.get('processed', 0),
            "total": task.get('total', 0),
            "eta_seconds": task.get('eta_seconds', 0),
            "avg_latency_ms": task.get('avg_latency_ms', 0)
        }
    }
    
    # åªæœ‰å½“ä»»åŠ¡å®Œæˆæ—¶ï¼Œæ‰è¿”å›ç»“æœæ•°æ® (é¿å…è½®è¯¢æ—¶ä¼ è¾“æ•°æ®è¿‡å¤§)
    if task['status'] == 'completed':
        response['results'] = task.get('results', [])
        
    return jsonify(response)

if __name__ == '__main__':
    print(f"ğŸš€ INFERENCE PRO SERVER starting on port {PORT}...")
    app.run(host='0.0.0.0', port=PORT, debug=False)
